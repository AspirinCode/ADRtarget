{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADR - target association identification\n",
    "GNU General Public License v3.0 - Robert Ietswaart\n",
    "\n",
    "### Citation:\n",
    "Robert Ietswaart<sup>\\*,#</sup>, Seda Arat<sup>\\*,#</sup>, Amanda X. Chen<sup>\\*</sup>, \n",
    "Saman Farahmand<sup>\\*</sup>, Bumjun Kim, William DuMouchel, \n",
    "Duncan Armstrong, Alexander Fekete, Jeffrey J. Sutherland<sup>#</sup>, Laszlo Urban<sup>#</sup>  \n",
    "*Machine learning guided association of adverse drug reactions with in vitro target-based \n",
    "pharmacology* (2019), [BioRxiv; 750950](https://www.biorxiv.org/content/10.1101/750950v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os.path\n",
    "import re\n",
    "from scipy.stats import ttest_ind, ttest_rel\n",
    "from statsmodels.stats.multitest import fdrcorrection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"~/ADRtarget/data/\"\n",
    "filename = 'mergedassaynumber-to-onewordname.csv'#NB: this file is not publicly available due to legal restrictions from Novartis.\n",
    "#Sample mapping file to indicate the format:\n",
    "# filename = 'sample_assayID-to-gene.csv'\n",
    "\n",
    "targets = pd.read_csv(path+filename)\n",
    "\n",
    "for i in targets.index:\n",
    "    A = re.sub('merged_', '', targets['Number'][i])\n",
    "    targets.loc[i,'Number'] = re.sub('_', '.', A)\n",
    "    if not str(targets['Type of Assay'][i]) == 'nan':\n",
    "        targets.loc[i,'Name']=targets['Name'][i]+'.'+targets['Type of Assay'][i]\n",
    "                 \n",
    "targets = targets.drop('Type of Assay',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine associations for each replicate run individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 'HLGT'\n",
    "diff_SEEDS = [112358,2662851,49,5332728]\n",
    "N_reps = 5\n",
    "\n",
    "for i in range(N_reps):\n",
    "    if i == 0:\n",
    "        path='~/ADRtarget/data/'+level+'/'\n",
    "    else:\n",
    "        seed=diff_SEEDS[i-1]\n",
    "        path='~/ADRtarget/data/'+level+'_diffSeed_'+str(seed)+'/'\n",
    "    filename='Features_ADRs_predictions.csv'            \n",
    "    df = pd.read_csv(path+filename,index_col=0)#assay classes are indices\n",
    "    colnames=list(df.columns)\n",
    "    N_ADR_terms=len(colnames)\n",
    "\n",
    "    cl = {'vals':['0-3','3-30','>30']}\n",
    "    cl = pd.DataFrame(cl)\n",
    "    N_cat = 3\n",
    "\n",
    "    output_dict = dict()\n",
    "    cl_dict = {1:'0-3uM',2:'3-30uM',3:'>30uM'}\n",
    "\n",
    "    for j in range(1,N_ADR_terms+1):\n",
    "        cn = colnames[j-1]#NB: ADR list is 1 based, python 0-based \n",
    "        print(j,cn)\n",
    "        if i == 0:\n",
    "            path = '~/ADRtarget/data/'+level+'/Importance_Gini/'\n",
    "        else:\n",
    "            seed = diff_SEEDS[i-1]\n",
    "            path = '~/ADRtarget/data/'+level+'_diffSeed_'+str(seed)+'/Importance_Gini/'\n",
    "            \n",
    "        filename = 'imp_for_ADR-'+str(j)+'_signf.csv'\n",
    "        if os.path.isfile(path+filename):\n",
    "            gini = pd.read_csv(path+filename)\n",
    "            gini = gini.rename(columns={'Unnamed: 0': 'cl', 'x':'gini_score'})\n",
    "\n",
    "            assay_ID = dict()#get assays in gini in useable format\n",
    "            for a_cl in gini['cl']:\n",
    "                temp = re.sub('A', '', a_cl)\n",
    "                temp = re.sub('CL', '', temp)\n",
    "                temp = re.split('_',temp)\n",
    "                a_key = temp[0]\n",
    "                try:\n",
    "                    assay_ID[a_key].append(a_cl)\n",
    "                except KeyError:\n",
    "                    assay_ID[a_key] = [a_cl]\n",
    "\n",
    "            for a in assay_ID.keys():\n",
    "                tar = targets['Name'][targets['Number']==a].values[0]\n",
    "                if len(assay_ID[a]) > 1:#at least 2 classes were significant: now you can compare assay results\n",
    "                    imp_cl = []\n",
    "                    for c in range(1,N_cat+1):#class index 1-based, python 0-based\n",
    "                        if 'A'+a+'_CL'+str(c) in assay_ID[a]:\n",
    "                            imp_cl.append(True)\n",
    "                        else:\n",
    "                            imp_cl.append(False)\n",
    "\n",
    "                    if (sum(df.loc[assay_ID[a],cn]) > 0):#if sign classes have non-zero prob (=model outputs)\n",
    "                        no_corr_test_prob = -1\n",
    "                        no_corr_test_bool = True\n",
    "                        #build csv file with same results\n",
    "                        print('Accepted: Assay:',a,': ',tar,' level',level,'term:',j,cn)\n",
    "                        try:\n",
    "                            output_dict['ADR'].append(cn.replace('.',' '))\n",
    "                            output_dict['target'].append(tar)\n",
    "                            output_dict['assay'].append(a)\n",
    "                            for c in range(1,N_cat+1):\n",
    "                                temp_cl = 'A'+a+'_CL'+str(c)\n",
    "                                if temp_cl in assay_ID[a]:\n",
    "                                    #test for no_correlation\n",
    "                                    if no_corr_test_prob == -1:\n",
    "                                        no_corr_test_prob = df.loc[temp_cl,cn]\n",
    "                                    elif no_corr_test_prob == df.loc[temp_cl,cn]:\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        no_corr_test_prob = df.loc[temp_cl,cn]\n",
    "                                        no_corr_test_bool = False\n",
    "                                    output_dict[cl_dict[c]].append(df.loc[temp_cl,cn])\n",
    "                                else:\n",
    "                                    output_dict[cl_dict[c]].append(np.nan)\n",
    "                            output_dict['no_corr'].append(no_corr_test_bool)\n",
    "                        except KeyError:\n",
    "                            output_dict['ADR'] = [cn.replace('.',' ')]\n",
    "                            output_dict['target'] = [tar]\n",
    "                            output_dict['assay'] = [a]\n",
    "                            for c in range(1,N_cat+1):\n",
    "                                temp_cl='A'+a+'_CL'+str(c)\n",
    "                                if temp_cl in assay_ID[a]:\n",
    "                                    #test for no_correlation\n",
    "                                    if no_corr_test_prob == -1:\n",
    "                                        no_corr_test_prob = df.loc[temp_cl,cn]\n",
    "                                    elif no_corr_test_prob == df.loc[temp_cl,cn]:\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        no_corr_test_prob = df.loc[temp_cl,cn]\n",
    "                                        no_corr_test_bool = False\n",
    "                                    output_dict[cl_dict[c]] = [df.loc[temp_cl,cn]]\n",
    "                                else:\n",
    "                                    output_dict[cl_dict[c]] = [np.nan]\n",
    "                            output_dict['no_corr'] = [no_corr_test_bool]\n",
    "                    else:\n",
    "                        print('Zeros only, Assay:',a,': ',tar,' level',level,'term:',j,cn) \n",
    "                else:\n",
    "                    print('One class significant only, Assay:',a,': ',tar,' level',level,'term:',j,cn)\n",
    "        else:\n",
    "            print('Gini file for ADR ',j,cn,' does not exist.', version_model,level)\n",
    "\n",
    "    if i == 0:\n",
    "        path = '~/ADRtarget/data/'+level+'/'\n",
    "        filename = level+'_ADR_target_assoc.csv'\n",
    "    else:\n",
    "        seed=diff_SEEDS[i-1]\n",
    "        path='~/ADRtarget/data/'+level+'_diffSeed_'+str(seed)+'/'\n",
    "        filename = level+'_ADR_target_assoc_seed'+str(seed)+'.csv'\n",
    "    \n",
    "    output_df=pd.DataFrame(output_dict)\n",
    "    output_df.to_csv(os.path.join(path,filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility analysis between 5 replicate runs\n",
    "identify for which ADRs the model only predicts zeros: exclude those ADRs (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADR_excl=dict()\n",
    "for i in range(N_reps):\n",
    "    filename='summ_metrics_ADRs.csv'\n",
    "    if i == 0:\n",
    "        path='~/ADRtarget/data/'+level+'/'\n",
    "        MetADR = pd.read_csv(path+filename,index_col=0)\n",
    "    else:\n",
    "        seed=diff_SEEDS[i-1]\n",
    "        path='~/ADRtarget/data/'+level+'_diffSeed_'+str(seed)+'/'\n",
    "        MetADR_OLD=copy.deepcopy(MetADR['ADR'])\n",
    "        MetADR = pd.read_csv(path+filename,index_col=0)\n",
    "        MetADR['ADR']=MetADR_OLD\n",
    "    MetADR_nonan=MetADR[~np.isnan(MetADR['precision'])]\n",
    "    MetADR_nonan['Pos']=MetADR_nonan['TP']+MetADR_nonan['FN']\n",
    "\n",
    "    ADR_excl[i]=MetADR[np.isnan(MetADR['precision'])]['ADR']\n",
    "    ADR_excl[i]=list(ADR_excl[i])\n",
    "    for j in range(len(ADR_excl[i])):\n",
    "        ADR_excl[i][j]=re.sub('\\(|\\)|,|\\'|-',' ',ADR_excl[i][j])\n",
    "    print(len(ADR_excl[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use meddra sheet to get mapping from HLGT to SOC level for ordering in heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='~/ADRtarget/data/'\n",
    "filename='meddr1a_full_12092006.xlsx'\n",
    "meddra = pd.read_excel(path+filename)\n",
    "\n",
    "HLGT2SOC=dict()\n",
    "for index, arow in meddra.iterrows():\n",
    "    hlgt = str(arow['HLGT_TXT']).lower()\n",
    "    hlgt=re.sub('\\(|\\)|,|\\'|-',' ',hlgt)\n",
    "    try:\n",
    "        HLGT2SOC[hlgt]\n",
    "    except KeyError:\n",
    "        HLGT2SOC[hlgt] = str(arow['SOC_TXT']).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all unfiltered target-ADRs from individual replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT=dict()\n",
    "for i in range(N_reps):\n",
    "    if i == 0:\n",
    "        path = '~/ADRtarget/data/'+level+'/'\n",
    "        filename = level+'_ADR_target_assoc.csv'\n",
    "    else:\n",
    "        seed=diff_SEEDS[i-1]\n",
    "        path='~/ADRtarget/data/'+level+'_diffSeed_'+str(seed)+'/'\n",
    "        filename = level+'_ADR_target_assoc_seed'+str(seed)+'.csv' \n",
    "    AT[i]= pd.read_csv(path+filename,index_col=0)\n",
    "\n",
    "for i in range(N_reps):\n",
    "    SOC=[]\n",
    "    for j in AT[i].index:\n",
    "        SOC.append(HLGT2SOC[AT[i]['ADR'][j]])\n",
    "    AT[i].insert(loc=1,column='SOC',value=SOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out ADRs from AT\n",
    "Filter out ADRs that have no predictive value (ADR_excl)\n",
    "\n",
    "Also filter out the following SOC classes: \n",
    "\n",
    "-general disorders and administration site conditions\n",
    "\n",
    "-injury, poisoning and procedural complications\n",
    "\n",
    "-poisoning and procedural complications\n",
    "\n",
    "-surgical and medical procedures\n",
    "\n",
    "-neoplasms benign, malignant and unspecified (incl cysts and polyps)\n",
    "\n",
    "-investigations\n",
    "\n",
    "-social circumstances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOC_excl=['surgical and medical procedures',\n",
    "        'general disorders and administration site conditions',\n",
    "        'injury, poisoning and procedural complications',\n",
    "        'investigations',\n",
    "        'poisoning and procedural complications',\n",
    "        'neoplasms benign, malignant and unspecified (incl cysts and polyps)',\n",
    "        'social circumstances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_reps):\n",
    "    AT[i]=AT[i][~AT[i]['ADR'].isin(ADR_excl[i])]\n",
    "    AT[i]=AT[i][~AT[i]['SOC'].isin(SOC_excl)]\n",
    "    AT[i]=AT[i].sort_values(by=['SOC','ADR','target'])\n",
    "    AT[i]=AT[i].drop(['no_corr'],axis=1)\n",
    "    AT[i]=AT[i].reindex(columns=['SOC','ADR','target','assay','0-3uM','3-30uM','>30uM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge (union = outer) to see overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATM = pd.merge(AT[0],AT[1], on=['SOC','ADR','target','assay'],how='outer')\n",
    "ATM.columns = ['SOC','ADR','target','assay','0:0-3uM','0:3-30uM','0:>30uM','1:0-3uM','1:3-30uM','1:>30uM']\n",
    "for i in range(2,N_reps):\n",
    "    ATM= pd.merge(ATM,AT[i], on=['SOC','ADR','target','assay'],how='outer')\n",
    "    colnames={'0-3uM':str(i)+':0-3uM','3-30uM':str(i)+':3-30uM','>30uM':str(i)+':>30uM'}\n",
    "    ATM=ATM.rename(columns=colnames) \n",
    "    \n",
    "ATM=ATM.sort_values(by=['SOC','ADR','target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct={1:[],2:[],3:[]}\n",
    "cl_dict={1:'0-3uM',2:'3-30uM',3:'>30uM'}\n",
    "for j in ATM.index:\n",
    "    for cl in cl_dict.keys():\n",
    "        count=0\n",
    "        for i in range(N_reps):\n",
    "            if not np.isnan(ATM[str(i)+':'+cl_dict[cl]][j]):\n",
    "                count+=1\n",
    "        ct[cl].append(count)\n",
    "ATM.insert(loc=4,column='N:'+cl_dict[1],value=ct[1])\n",
    "ATM.insert(loc=5,column='N:'+cl_dict[2],value=ct[2])\n",
    "ATM.insert(loc=6,column='N:'+cl_dict[3],value=ct[3])\n",
    "\n",
    "#get averages over reps\n",
    "ct={1:[],2:[],3:[]}\n",
    "se={1:[],2:[],3:[]}\n",
    "for j in ATM.index:\n",
    "    for cl in cl_dict.keys():\n",
    "        ct[cl].append( \\\n",
    "    np.mean(ATM.loc[j,['0:'+cl_dict[cl],'1:'+cl_dict[cl],'2:'+cl_dict[cl],'3:'+cl_dict[cl],'4:'+cl_dict[cl]]]))\n",
    "        se[cl].append( \\\n",
    "    np.std(ATM.loc[j,['0:'+cl_dict[cl],'1:'+cl_dict[cl],'2:'+cl_dict[cl],'3:'+cl_dict[cl],'4:'+cl_dict[cl]]]) \\\n",
    "                      / np.sqrt(ATM.loc[j,'N:'+cl_dict[cl]]))    \n",
    "ATM.insert(loc=7,column='mean:'+cl_dict[1],value=ct[1])\n",
    "ATM.insert(loc=8,column='mean:'+cl_dict[2],value=ct[2])\n",
    "ATM.insert(loc=9,column='mean:'+cl_dict[3],value=ct[3])\n",
    "ATM.insert(loc=10,column='sem:'+cl_dict[1],value=se[1])\n",
    "ATM.insert(loc=11,column='sem:'+cl_dict[2],value=se[2])\n",
    "ATM.insert(loc=12,column='sem:'+cl_dict[3],value=se[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out non-significant associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_FDR = 0.1\n",
    "pvals = []\n",
    "cl_dict = {1:'0-3uM',2:'3-30uM',3:'>30uM'}\n",
    "\n",
    "#filter out ADR-targets that were only found 1: not reproducible\n",
    "temp = ATM[['N:'+cl_dict[1],'N:'+cl_dict[2],'N:'+cl_dict[3]]]>1\n",
    "ATM = ATM[temp.any(axis=1)]\n",
    "\n",
    "for j in ATM.index:\n",
    "    count=0\n",
    "    data_sample1=[]\n",
    "    data_sample2=[]\n",
    "    for cl in cl_dict.keys():\n",
    "        if ATM.loc[j,'N:'+cl_dict[cl]]>1: \n",
    "            if count==0:\n",
    "                data_sample1=list(ATM.loc[j,['0:'+cl_dict[cl],'1:'+cl_dict[cl],'2:'+cl_dict[cl],'3:'+cl_dict[cl],'4:'+cl_dict[cl]]])\n",
    "                count+=1\n",
    "            elif count==1:\n",
    "                data_sample2=list(ATM.loc[j,['0:'+cl_dict[cl],'1:'+cl_dict[cl],'2:'+cl_dict[cl],'3:'+cl_dict[cl],'4:'+cl_dict[cl]]])\n",
    "    stat,pval=ttest_ind(data_sample1,data_sample2,nan_policy='omit')#independent samples t test\n",
    "\n",
    "    if np.isnan(stat):#nan data so drop and do not test for significance\n",
    "        print(stat,pval,'drop',j)\n",
    "        ATM=ATM.drop(j) \n",
    "    else:\n",
    "        pvals.append(pval)\n",
    "\n",
    "BOOL, qvals = fdrcorrection(pvals, alpha=1)\n",
    "ATM.insert(loc=7,column='pval',value=pvals)\n",
    "ATM.insert(loc=8,column='padj',value=qvals)\n",
    "ATM = ATM[ATM['padj']<alpha_FDR]\n",
    "\n",
    "ATM = ATM.sort_values(by=['SOC','ADR','target'])\n",
    "path = '~/ADRtarget/data/'\n",
    "filename = level+'_ADR_target_assoc_final.csv'\n",
    "ATM.to_csv(path+filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
